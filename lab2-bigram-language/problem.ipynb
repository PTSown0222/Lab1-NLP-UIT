{"cells":[{"cell_type":"markdown","id":"5504a5d7","metadata":{},"source":["### Bigram Language Models\n","\n","- Công thức bigram cơ bản:\n","\n","$P(w_2 \\mid w_1) = \\frac{\\operatorname{count}(w_1, w_2)}{\\operatorname{count}(w_1)}$\n","\n","- Bigram với smoothing:\n","\n","$P(w_2 \\mid w_1) = \\frac{\\operatorname{count}(w_1, w_2) + \\alpha}{\\operatorname{count}(w_1) + \\alpha \\times V}$\n","\n"]},{"cell_type":"code","execution_count":70,"id":"52607b19-07f5-4aea-ae3d-6b2d19fe4a10","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2861,"status":"ok","timestamp":1711068360943,"user":{"displayName":"Nguyễn Đức Vũ","userId":"07288779277842550568"},"user_tz":-420},"outputId":"2522d21c-a9b8-45be-aca1-e0787cae0bf2"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /Users/theson/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to\n","[nltk_data]     /Users/theson/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]}],"source":["from typing import List, Tuple, Dict\n","import numpy as np\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from collections import Counter, defaultdict\n","\n","# Ensure necessary NLTK tokenizer models are available\n","nltk.download(\"punkt\")\n","nltk.download('punkt_tab')\n","\n","class Tokenizer:\n","    def __init__(self, tokenize_type: str = \"basic\", lowercase: bool = False):\n","        self.lowercase = lowercase\n","        self.type = tokenize_type\n","        self.vocab = []  # Empty vocabulary list\n","\n","    def basicTokenize(self, string: str) -> List[str]:\n","        # Tokenizes input string by splitting on whitespace\n","        ### BEGIN SOLUTION\n","        tokens = string.split()\n","        return tokens\n","        ### END SOLUTION\n","\n","    def nltkTokenize(self, string: str) -> List[str]:\n","        # Tokenizes input string using NLTK's word tokenizer\n","        ### BEGIN SOLUTION\n","        tokens = word_tokenize(string)\n","        return tokens\n","        ### END SOLUTION\n","\n","    def tokenize(self, string: str) -> List[str]:\n","        # Tokenizes string and updates vocabulary with unique words\n","        if self.lowercase:\n","            string = string.lower()\n","        tokens = self.basicTokenize(string) if self.type == \"basic\" else self.nltkTokenize(string)\n","        self.vocab += [w for w in set(tokens) if w not in self.vocab]\n","        return tokens\n","\n","    def countTopWords(self, words: List[str], k: int) -> List[Tuple[str, int]]:\n","        # Returns the top k most common words\n","        ### BEGIN SOLUTION\n","        word_counts = Counter(words)\n","        top_k_words = word_counts.most_common(k)\n","        return top_k_words\n","        ### END SOLUTION"]},{"cell_type":"code","execution_count":71,"id":"9f2ca75f-d441-4712-868f-ba15b786854d","metadata":{},"outputs":[],"source":["class BiGramLanguageModel:\n","    def __init__(self, vocab: List[str], smoothing: str = None, smoothing_param: float = None): pass\n","    def computeBigramProb(self): pass\n","    def computeBigramProbAddAlpha(self, alpha: float = 0.001): pass\n","    def train(self, corpus: List[str]): pass\n","    def test(self, corpus: List[str]) -> float: pass"]},{"cell_type":"code","execution_count":72,"id":"eec09244-5606-4f5a-ac10-bd47697f43ac","metadata":{},"outputs":[],"source":["def bigram_init(self, vocab, smoothing=None, smoothing_param=None):\n","    self.vocab = vocab\n","    self.token_to_idx = {word: i for i, word in enumerate(vocab)}\n","    self.smoothing = smoothing\n","    self.smoothing_param = smoothing_param\n","    self.bi_counts = None\n","    self.bi_prob = None\n","    assert smoothing is None or smoothing_param is not None, \"Smoothing parameters must be set correctly.\"\n"]},{"cell_type":"code","execution_count":73,"id":"bbf16119-9740-4d6b-aaf7-2a30b543b6e1","metadata":{},"outputs":[],"source":["def bigram_compute_prob(self):\n","    # Tính P(w2|w1) từ bi_counts\n","    V = len(self.vocab)\n","    self.bi_prob = np.zeros((V, V), dtype=float)\n","    \n","    # Tính unigram counts từ bi_counts\n","    unigram_counts = self.bi_counts.sum(axis=1)  # Sum across columns\n","    \n","    for i in range(V):\n","        if unigram_counts[i] > 0:\n","            self.bi_prob[i, :] = self.bi_counts[i, :] / unigram_counts[i]\n","        else:\n","            self.bi_prob[i, :] = 0  # Tránh chia cho 0\n","    \n","    return self.bi_prob"]},{"cell_type":"code","execution_count":74,"id":"909f938e-ddb7-4516-8f1b-c9a94557987f","metadata":{},"outputs":[],"source":["def bigram_compute_prob_add_alpha(self, alpha=1e-3):\n","    # Add-alpha smoothing\n","    V = len(self.vocab)\n","    self.bi_prob = np.zeros((V, V), dtype=float)\n","    \n","    # Tính unigram counts\n","    unigram_counts = self.bi_counts.sum(axis=1)\n","    \n","    for i in range(V):\n","        # Add-alpha smoothing formula\n","        self.bi_prob[i, :] = (self.bi_counts[i, :] + alpha) / (unigram_counts[i] + alpha * V)\n","    \n","    return self.bi_prob"]},{"cell_type":"code","execution_count":75,"id":"86ebf856-6826-4716-a9b2-84559c5d5195","metadata":{},"outputs":[],"source":["def bigram_train(self, corpus):\n","    self.bi_counts = np.zeros((len(self.vocab), len(self.vocab)), dtype=float)\n","    corpus_indices = [self.token_to_idx[w] for w in corpus]\n","    for i in range(len(corpus_indices) - 1):\n","        self.bi_counts[corpus_indices[i]][corpus_indices[i + 1]] += 1\n","    if self.smoothing == \"addAlpha\":\n","        self.computeBigramProbAddAlpha(self.smoothing_param)\n","    else:\n","        self.computeBigramProb()"]},{"cell_type":"code","execution_count":76,"id":"4df38f40-94ed-4378-a88a-c345315da51d","metadata":{},"outputs":[],"source":["def bigram_test(self, corpus):\n","    logprob = 0.0\n","    eps = 1e-10\n","    corpus_indices = [self.token_to_idx[w] for w in corpus]\n","    for i in range(len(corpus_indices) - 1):\n","        prob = self.bi_prob[corpus_indices[i], corpus_indices[i + 1]]\n","        logprob += np.log(prob + eps)\n","    logprob /= len(corpus_indices) - 1\n","    return np.exp(-logprob)"]},{"cell_type":"code","execution_count":77,"id":"3ccd0096-b74a-49a9-84f0-28b9fd7283aa","metadata":{},"outputs":[],"source":["BiGramLanguageModel.__init__ = bigram_init\n","BiGramLanguageModel.computeBigramProb = bigram_compute_prob\n","BiGramLanguageModel.computeBigramProbAddAlpha = bigram_compute_prob_add_alpha\n","BiGramLanguageModel.train = bigram_train\n","BiGramLanguageModel.test = bigram_test"]},{"cell_type":"code","execution_count":78,"id":"0ea0e8e4-ed26-4fe4-b78c-13a78ad092ec","metadata":{},"outputs":[],"source":["def readCorpus(filename: str, tokenizer: Tokenizer) -> List[str]:\n","    # Reads and tokenizes the corpus from a file\n","    ### BEGIN SOLUTION\n","    with open(filename, 'r', encoding='utf-8') as file:\n","        text = file.read()\n","    tokens = tokenizer.tokenize(text)\n","    return tokens\n","    ### END SOLUTION"]},{"cell_type":"code","execution_count":79,"id":"2dc02b36-30f4-46e9-8555-4c54a1d73490","metadata":{},"outputs":[],"source":["def runLanguageModel(train_corpus: List[str], val_corpus: List[str], tokenizer: Tokenizer, smoothing_type: str = None, smoothing_param: float = 0.0) -> Dict[str, float]:\n","    # Trains and tests the language model, returning key metrics\n","    lm = BiGramLanguageModel(tokenizer.vocab, smoothing=smoothing_type, smoothing_param=smoothing_param)\n","    lm.train(train_corpus)\n","    return {\"train_ppl\": lm.test(train_corpus), \"val_ppl\": lm.test(val_corpus)}"]},{"cell_type":"code","execution_count":80,"id":"22de9261-fc4b-4e5d-b40b-e1fd49170909","metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1711068360944,"user":{"displayName":"Nguyễn Đức Vũ","userId":"07288779277842550568"},"user_tz":-420}},"outputs":[],"source":["# Initialize tokenizers with basic and NLTK options, both set to lowercase.\n","basic_tokenizer = Tokenizer(tokenize_type='basic', lowercase=True)\n","nltk_tokenizer = Tokenizer(tokenize_type='nltk', lowercase=True)"]},{"cell_type":"code","execution_count":81,"id":"7c9eb06c-e0a3-4191-b899-2378f50b2b20","metadata":{"executionInfo":{"elapsed":13437,"status":"ok","timestamp":1711068374376,"user":{"displayName":"Nguyễn Đức Vũ","userId":"07288779277842550568"},"user_tz":-420}},"outputs":[],"source":["# Read and tokenize the training and validation corpora using the basic tokenizer.\n","train_corpus = readCorpus('./data/train.txt', basic_tokenizer)\n","val_corpus = readCorpus('./data/val.txt', basic_tokenizer)\n","\n","# Example of using the NLTK tokenizer for comparison (unused in final results).\n","train_corpus_nltk = readCorpus('./data/train.txt', nltk_tokenizer)\n","val_corpus_nltk = readCorpus('./data/val.txt', nltk_tokenizer)"]},{"cell_type":"code","execution_count":82,"id":"4676580e-677d-4c7a-bab9-3b772e5d03a0","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1711068374377,"user":{"displayName":"Nguyễn Đức Vũ","userId":"07288779277842550568"},"user_tz":-420},"outputId":"43c9c3da-7271-42e9-fb8c-03f6b05709db"},"outputs":[{"data":{"text/plain":["[('unk', 61019),\n"," ('the', 45302),\n"," ('of', 25379),\n"," ('and', 18067),\n"," ('to', 16515),\n"," ('a', 14371),\n"," ('in', 14231),\n"," ('is', 7466),\n"," ('that', 6484),\n"," ('for', 6434)]"]},"execution_count":82,"metadata":{},"output_type":"execute_result"}],"source":["# Get top 10 frequent words and counts from train_corpus with basic_tokenizer.\n","### BEGIN PUBLIC TESTS\n","basic_tokenizer.countTopWords(train_corpus, k=10)\n","### END PUBLIC TESTS"]},{"cell_type":"code","execution_count":83,"id":"9ac6707f-8ba7-4773-93f6-0ebca633c0e1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1711068374377,"user":{"displayName":"Nguyễn Đức Vũ","userId":"07288779277842550568"},"user_tz":-420},"outputId":"25cc7e4f-eb98-45a2-f328-4f6eb1f377d4"},"outputs":[{"data":{"text/plain":["[('unk', 61019),\n"," ('the', 45885),\n"," ('of', 25427),\n"," (',', 23570),\n"," ('and', 18346),\n"," ('.', 17532),\n"," ('to', 16606),\n"," ('a', 14721),\n"," ('in', 14358),\n"," ('is', 7702)]"]},"execution_count":83,"metadata":{},"output_type":"execute_result"}],"source":["# Get top 10 frequent words and counts from train_corpus_nltk with nltk_tokenizer.\n","### BEGIN PUBLIC TESTS\n","nltk_tokenizer.countTopWords(train_corpus_nltk, k=10)\n","### END PUBLIC TESTS"]},{"cell_type":"code","execution_count":84,"id":"4056da6a-e1d3-44e5-bfbb-98bdfc6fb1cd","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7622,"status":"ok","timestamp":1711068382544,"user":{"displayName":"Nguyễn Đức Vũ","userId":"07288779277842550568"},"user_tz":-420},"outputId":"a40640c2-8344-434b-a1b4-f5968db35f9c"},"outputs":[{"data":{"text/plain":["{'train_ppl': 69.87840111738812, 'val_ppl': 58435.97213541714}"]},"execution_count":84,"metadata":{},"output_type":"execute_result"}],"source":["# Run the language model with the basic tokenizer and without smoothing.\n","### BEGIN PUBLIC TESTS\n","runLanguageModel(train_corpus, val_corpus,\n","                 tokenizer=basic_tokenizer)\n","### END PUBLIC TESTS"]},{"cell_type":"code","execution_count":85,"id":"f9a423aa-e8d7-42d5-9ee8-c765e5e2263d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5845,"status":"ok","timestamp":1711068388387,"user":{"displayName":"Nguyễn Đức Vũ","userId":"07288779277842550568"},"user_tz":-420},"outputId":"6579fea6-548a-4916-d017-d70f16f3bc3a"},"outputs":[{"data":{"text/plain":["{'train_ppl': 69.68966075535424, 'val_ppl': 9285.574606771734}"]},"execution_count":85,"metadata":{},"output_type":"execute_result"}],"source":["# Run the language model with the nltk tokenizer and without smoothing.\n","### BEGIN PUBLIC TESTS\n","runLanguageModel(train_corpus_nltk, val_corpus_nltk,\n","                 tokenizer=nltk_tokenizer)\n","### END PUBLIC TESTS"]},{"cell_type":"code","execution_count":86,"id":"d03d8f8a-7c13-48f7-8633-3fbe9097e760","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7885,"status":"ok","timestamp":1711068396265,"user":{"displayName":"Nguyễn Đức Vũ","userId":"07288779277842550568"},"user_tz":-420},"outputId":"dc6893e3-4fef-4b8d-8e2b-ad1f7bddd9fc"},"outputs":[{"data":{"text/plain":["{'train_ppl': 74.15013253687664, 'val_ppl': 2216.4589207208182}"]},"execution_count":86,"metadata":{},"output_type":"execute_result"}],"source":["# Run the language model with the basic tokenizer and with smoothing.\n","runLanguageModel(train_corpus, val_corpus,\n","                 tokenizer=basic_tokenizer, smoothing_type='addAlpha', smoothing_param=10e-5)"]},{"cell_type":"code","execution_count":87,"id":"72b44238-9907-4a88-930b-b983ab3b23fe","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5739,"status":"ok","timestamp":1711068402001,"user":{"displayName":"Nguyễn Đức Vũ","userId":"07288779277842550568"},"user_tz":-420},"outputId":"a98d4657-0def-4831-e550-b0b5091ebe1c"},"outputs":[{"data":{"text/plain":["{'train_ppl': 71.60376054214575, 'val_ppl': 914.026630299532}"]},"execution_count":87,"metadata":{},"output_type":"execute_result"}],"source":["# Run the language model with the nltk tokenizer and with smoothing.\n","runLanguageModel(train_corpus_nltk, val_corpus_nltk,\n","                 tokenizer=nltk_tokenizer, smoothing_type='addAlpha', smoothing_param=10e-5)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.18"}},"nbformat":4,"nbformat_minor":5}
